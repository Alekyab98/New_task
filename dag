from datetime import datetime, timedelta
from functools import partial
from airflow import DAG
import os
import sys
import yaml
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,
)
from airflow.operators.dummy import DummyOperator


folder = 'ericsson_upf'
BASE_DIR = "/home/airflow/gcs/dags/vz-it-gudv-dtwndo-0"
sys.path.append(f"{BASE_DIR}/{folder}/python")

from DO_utils import publishLog, create_do_dict

project = os.environ['GCP_PROJECT']
with open(f"{BASE_DIR}/{folder}/config/base_config.yml", 'r') as file:
    base_config = yaml.full_load(file)

with open(f"{BASE_DIR}/{folder}/config/aether_core_upf_performance.yml", 'r') as file:
    dag_config = yaml.full_load(file)

config_values = {}

filtered_base_dict = dict(filter(lambda elem: elem[0] == project, base_config.items()))
filtered_dict = dict(filter(lambda elem: elem[0] == project, dag_config.items()))

if len(filtered_base_dict) > 0:
    base_value = filtered_base_dict[project][0]
    config_values = {**config_values, **base_value}
else:
    print("No config found exiting..")
    sys.exit(-1)
if len(filtered_dict) > 0:
    app_value = filtered_dict[project][0]
    config_values = {**config_values, **app_value}
else:
    print("No config found exiting..")
    sys.exit(-1)

GCP_PROJECT_ID = config_values['gcp_project']
bq_connection_id = config_values['google_cloud_conn_id']
region = config_values['region']
DAG_ID = config_values['dag_id']
base_directory = config_values['base_directory']
env = config_values['env']
dataset_id = config_values['dataset_id']
stored_proc = config_values['stored_proc']
table_name = config_values['table_name']
schedule_interval = config_values['schedule_interval']
failure_email_alert_distro = config_values['failure_email_alert_distro']
window_hour = config_values['window_hour']
window_interval = config_values['window_interval']




process_ts = '{{ data_interval_end.strftime("%Y-%m-%d %H:%M:%S") }}'
trans_ts = '{{ data_interval_end.subtract(hours=1).strftime("%Y-%m-%d %H:%M:%S") }}'

do_dict = create_do_dict(config_values)



default_args = {
    'owner': 'dtwin',
    'depends_on_past': False,
    'start_date': datetime(year=2024, month=9, day=6, hour=00, minute=00),
    'email': [failure_email_alert_distro],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': timedelta(minutes=3)
}


dag = DAG(
        dag_id=DAG_ID,
        schedule_interval=schedule_interval,
        catchup=True,
        default_args=default_args,
        description='This DAG call Stored Procedure for core_5g upf',
        concurrency=int(config_values['concurrency']),
        max_active_runs=int(config_values['max_active_runs']),
        tags=["dtwin","aether_core_5g"]
)

start = DummyOperator(task_id='start',
                      dag=dag,
                      on_success_callback=partial(publishLog, "PROGRESS", do_dict),
                      on_failure_callback=partial(publishLog, "FAILURE", do_dict))

call_aether_upf_performance_core = BigQueryInsertJobOperator(
        task_id="call_aether_upf_performance_core",
        dag=dag,
        gcp_conn_id=bq_connection_id,
        configuration={
                         "query": {
                              "query": f"CALL {dataset_id}.{stored_proc}('{process_ts}','{trans_ts}',{window_hour},{window_interval})",
                              "useLegacySql": False,
                              }
                         },
        on_failure_callback=partial(publishLog, "FAILURE", do_dict)
)


end = DummyOperator(task_id='end',
                    dag=dag,
                    on_success_callback=partial(publishLog, "SUCCESS", do_dict),
                    on_failure_callback=partial(publishLog, "FAILURE", do_dict))


start >> call_aether_upf_performance_core >> end
